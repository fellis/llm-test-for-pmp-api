# Single LLM backend: model chosen by MODEL_PROFILE (see config/models.json).
# Run: ./scripts/start.sh <profile>  (e.g. coding | reasoning | chat)
# Models are cached in ./models (Hugging Face cache), not re-downloaded.

services:
  llm:
    build: ./llm
    image: llm-backend
    container_name: llm-backend
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - MODEL_PROFILE=${MODEL_PROFILE:-coding}
      - CONFIG_PATH=/config/models.json
    volumes:
      - ./models:/root/.cache/huggingface
      - ./config:/config:ro
    ports:
      - "8002:8002"
    shm_size: "8gb"
    restart: unless-stopped

  api:
    build: ./api
    container_name: llm-api
    ports:
      - "8000:8000"
    environment:
      - BACKEND_URL=http://llm:8002
      - BACKEND_MODEL_ID=${BACKEND_MODEL_ID:-Qwen/Qwen2.5-Coder-14B-Instruct-AWQ}
    # Optional: mount auth.json for Bearer token auth
    # volumes:
    #   - ./auth.json:/app/auth.json
    restart: unless-stopped
